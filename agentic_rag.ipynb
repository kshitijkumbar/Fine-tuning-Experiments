{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import sys\n",
    "openai.api_key = os.getenv(\"OAI_KEY\")\n",
    "brave_key = os.getenv(\"BRAVE_KEY\")\n",
    "os.environ[\"OPENAI_API_KEY\"]= os.getenv(\"OAI_KEY\")\n",
    "client = OpenAI()  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import ArxivLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "docs = ArxivLoader(query =\"text query here\", load_max_docs=2).load()\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=350, chunk_overlap=50\n",
    ")\n",
    "\n",
    "chunked_docs = text_splitter.split_documents(docs)\n",
    "\n",
    "vector_store = FAISS.from_documents(\n",
    "    documents=chunked_docs,\n",
    "    embedding=OpenAIEmbeddings(model=\"text-embedding-3-small\"),\n",
    ")\n",
    "\n",
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "RAG_PROMPT= \"\"\"\\\n",
    "    Use the following context to answer the user's query. If you cannot answer the question, please respond with 'I do not know'\n",
    "    \n",
    "    Question:\n",
    "    {question}\n",
    "    \n",
    "    Context:\n",
    "    {context}\n",
    "\"\"\"\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_template(RAG_PROMPT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup Generation Model (GPT-3.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "openai_chat_model = ChatOpenAI(model=\"gpt-3.5-turbo\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: EXPLORE LCEL CHAINS\n",
    "\n",
    "from operator import itemgetter\n",
    "from langchain.schema.output_parser import SrcOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "rag_chain = (\n",
    "    {'context': itemgetter('question') | retriever, 'question': itemgetter('question')}\n",
    "    | RunnablePassthrough.assign(context=itemgetter('context'))\n",
    "    | {'response': rag_prompt | openai_chat_model, 'context': itemgetter('context')}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await rag_chain.ainvoke({\"question\": \"What is RAG?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
