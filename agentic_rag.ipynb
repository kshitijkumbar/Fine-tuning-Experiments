{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import sys\n",
    "openai.api_key = os.getenv(\"OAI_KEY\")\n",
    "brave_key = os.getenv(\"BRAVE_KEY\")\n",
    "os.environ[\"OPENAI_API_KEY\"]= os.getenv(\"OAI_KEY\")\n",
    "client = OpenAI()  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import ArxivLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "docs = ArxivLoader(query =\"text query here\", load_max_docs=2).load()\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=350, chunk_overlap=50\n",
    ")\n",
    "\n",
    "chunked_docs = text_splitter.split_documents(docs)\n",
    "\n",
    "vector_store = FAISS.from_documents(\n",
    "    documents=chunked_docs,\n",
    "    embedding=OpenAIEmbeddings(model=\"text-embedding-3-small\"),\n",
    ")\n",
    "\n",
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "RAG_PROMPT= \"\"\"\\\n",
    "    Use the following context to answer the user's query. If you cannot answer the question, please respond with 'I do not know'\n",
    "    \n",
    "    Question:\n",
    "    {question}\n",
    "    \n",
    "    Context:\n",
    "    {context}\n",
    "\"\"\"\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_template(RAG_PROMPT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup Generation Model (GPT-3.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "openai_chat_model = ChatOpenAI(model=\"gpt-3.5-turbo\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: EXPLORE LCEL CHAINS\n",
    "\n",
    "from operator import itemgetter\n",
    "from langchain.schema.output_parser import SrcOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "rag_chain = (\n",
    "    {'context': itemgetter('question') | retriever, 'question': itemgetter('question')}\n",
    "    | RunnablePassthrough.assign(context=itemgetter('context'))\n",
    "    | {'response': rag_prompt | openai_chat_model, 'context': itemgetter('context')}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await rag_chain.ainvoke({\"question\": \"What is RAG?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add tools for LangGraph implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.ddg_search import DuckDuckGoSearch\n",
    "from langchain_community.tools.arxiv.tool import ArxivQueryRun\n",
    "\n",
    "tools_list = [\n",
    "    DuckDuckGoSearch(),\n",
    "    ArxivQueryRun()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import ToolExecutor\n",
    "tool_executor = ToolExecutor(tools_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.utils.function_calling import convert_to_openai_function\n",
    "\n",
    "# Get deterministic outputs from gpt\n",
    "model = ChatOpenAI(temperature=0) \n",
    "functions = [convert_to_openai_function(tool) for tool in tools_list]\n",
    "model = model.bind_functions(functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Agent State Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, Sequence\n",
    "import operator\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add] #operator for documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create graph node functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import ToolInvocation\n",
    "import json\n",
    "from langchain_core.messages import FunctionMessage\n",
    "\n",
    "def call_model(state):\n",
    "    messages = state['messages']\n",
    "    response = model.invoke(messages)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "def call_tool(state):\n",
    "    last_message = state['messages'][-1]\n",
    "    \n",
    "    action = ToolInvocation (\n",
    "        tool = last_message.additional_kwargs['function_call']['name'],\n",
    "        tool_input = json.loads(\n",
    "            last_message.additional_kwargs['function_call']['arguments']\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    response = tool_executor.invoke(action)\n",
    "    \n",
    "    function_msg = FunctionMessage(content=str(response), name=action.tool)\n",
    "    \n",
    "    return {\"messages\": [function_msg]}      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "workflow.add_node(\"action\", call_tool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.set_entry_point(\"agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add condition check\n",
    "\n",
    "def should_continue(state):\n",
    "    last_msg = state['messages'][-1]\n",
    "    \n",
    "    if 'function_call' not in last_msg.additional_kwargs:\n",
    "        return 'end'\n",
    "    else:\n",
    "        return 'continue'\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    'agent',\n",
    "    should_continue,\n",
    "    {\n",
    "        'continue' : 'action',\n",
    "        'end' : END\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.add_edge('action', 'agent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Add RAG chain to graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_state_to_query(state_object):\n",
    "    return {'question' : state_object['messages'][-1].content}\n",
    "\n",
    "def convert_response_to_state(response):\n",
    "    return {'messages' : [response['response']]}\n",
    "\n",
    "langgraph_node_rag_chain = convert_state_to_query | rag_chain | convert_response_to_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "inputs = {\"messages\" : [HumanMessage(content=\"What is RAG in the context of Large Language Models? When did it break onto the scene?\")]}\n",
    "\n",
    "await langgraph_node_rag_chain.ainvoke(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally create RAG agent \n",
    "rag_agent = StateGraph(AgentState)\n",
    "\n",
    "rag_agent.add_node('agent', call_model)\n",
    "rag_agent.add_node('action', call_tool)\n",
    "rag_agent.add_node('first_action', langgraph_node_rag_chain)\n",
    "\n",
    "rag_agent.set_entry_point('first_action')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add check for full answer\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain.output_parsers.openai_tools import PydanticToolsParser\n",
    "from langchain_core.utils.function_calling import convert_to_openai_tool\n",
    "\n",
    "\n",
    "def is_fully_answered(state):\n",
    "    \n",
    "    question = state['messages'][0].content\n",
    "    answer = state['messages'][-1].content\n",
    "\n",
    "    class answered(BaseModel):\n",
    "        binary_score: str = Field(description=\"Fully answered: 'yes' or 'no'\")\n",
    "        \n",
    "    model = ChatOpenAI(model='gpt-4-turbo-preview', temperature=0)\n",
    "    \n",
    "    answered_tool = convert_to_openai_tool(answered)\n",
    "    \n",
    "    model = model.bind(\n",
    "        tools=[answered_tool],\n",
    "        tool_choice = {'type': 'function', 'function': {'name' : 'answered'}}\n",
    "    )\n",
    "    parser_tool = PydanticToolsParser(tools=[answered])\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"You will determine if the question is fully answered by the response.\\n\n",
    "        Question:\n",
    "        {question}\n",
    "        \n",
    "        Respose:\n",
    "        {answer}\n",
    "        \n",
    "        You will respond with either 'yes' or 'no'.\"\"\",\n",
    "        input_variables=['question','answer']\n",
    "    )\n",
    "\n",
    "    complete_answer_chain = prompt | model | parser_tool\n",
    "    response = complete_answer_chain.invoke({'question': question, 'answer': answer})\n",
    "\n",
    "    if response[0].binary_score == 'no':\n",
    "        return 'continue'\n",
    "\n",
    "    return'end'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_agent.add_conditional_edges(\n",
    "    'first_action',\n",
    "    is_fully_answered,\n",
    "    {\n",
    "        'continue' : 'agent',\n",
    "        'end' : END\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_agent.add_edge('action','agent')\n",
    "rag_agent_app = rag_agent.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
